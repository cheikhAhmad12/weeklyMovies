{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a7147daa-7828-495f-876e-f94d09adf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "af738697-d218-4530-8f3c-628693839c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    dataset=[]\n",
    "    with open(filename) as f:\n",
    "        line = f.readline()\n",
    "        cpt=1\n",
    "        skip=0\n",
    "        while line :\n",
    "            cleanline = clean_str(f.readline()).split()\n",
    "            if cleanline: \n",
    "                dataset.append(cleanline)\n",
    "            else: \n",
    "                line = f.readline()\n",
    "                skip+=1\n",
    "                continue\n",
    "            if limit > 0 and cpt >= limit: \n",
    "                break\n",
    "            line = f.readline()\n",
    "            cpt+=1        \n",
    "\n",
    "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "62588dd1-3422-431b-8b64-58676df31858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  imdb/imdb.neg  /  1  lines discarded\n",
      "Load  5000  lines from  imdb/imdb.pos  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM = 5000\n",
    "txtfile = \"imdb/imdb.neg\"  # path of the file containing positive reviews\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = \"imdb/imdb.pos\"  # path of the file containing negative reviews\n",
    "negtxt = loadTexts(txtfile,limit=LIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4646e2e6-a612-44c9-a14d-06499adbe53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 7000\n",
      "Dev set: 1500\n",
      "Test set: 1500\n"
     ]
    }
   ],
   "source": [
    "#Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = postxt + negtxt\n",
    "labels = [1] * len(postxt) + [0] * len(negtxt)\n",
    "txt_train, txt_temp, label_train, label_temp = train_test_split(data, labels, test_size=0.3, stratify=labels, random_state=42)\n",
    "txt_dev, txt_test, label_dev, label_test = train_test_split(txt_temp, label_temp, test_size=0.5, stratify=label_temp, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(txt_train)}\")\n",
    "print(f\"Dev set: {len(txt_dev)}\")\n",
    "print(f\"Test set: {len(txt_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f2ba754c-c5b7-45c6-96fa-2c7f5db079bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up dictionary\n",
    "def map_word(data):\n",
    "    dico={}\n",
    "    i = 1\n",
    "    for sentence in data : \n",
    "        for word in sentence :\n",
    "            if word not in dico:\n",
    "                dico[word]= i\n",
    "                i +=1\n",
    "    return dico\n",
    "my_dico= map_word(txt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6935cdfd-c63c-4ea2-88e9-3ae976cc2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data to tensors\n",
    "def convert_to_tensor(data, label, dico):\n",
    "    sentences_tensor= []\n",
    "    labels_tensor=[]\n",
    "    for sentence, sublabel in zip(data, label) : \n",
    "        sentence_int = [dico[x] for x in sentence if x in dico]\n",
    "        if sentence_int :\n",
    "            sentences_tensor.append(torch.tensor(sentence_int, dtype=torch.long))\n",
    "            labels_tensor.append(torch.tensor(sublabel, dtype=torch.long))\n",
    "    return sentences_tensor, labels_tensor\n",
    "\n",
    "train_sentence, train_label = convert_to_tensor(txt_train, label_train, my_dico)\n",
    "test_sentence, test_label=convert_to_tensor(txt_test, label_test, my_dico)\n",
    "dev_sentence, dev_label=convert_to_tensor(txt_dev, label_dev, my_dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4ae3ed5d-cce8-47af-8d48-478c062bde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG of word classifier without hidden layer\n",
    "class CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Optional: hidden layer\n",
    "        self.output = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        bag_of_words = embeddings.mean(dim=1)\n",
    "        output = self.output(bag_of_words)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5ae7bfe2-ab1d-4ff9-9ec8-a8d379f9213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAG of word classifier with 1 hidden layer\n",
    "class CBOW_classifier2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(CBOW_classifier2, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        bag_of_words = embeddings.mean(dim=1)\n",
    "        hidden_out = self.activation(self.hidden(bag_of_words)) \n",
    "        output = self.output(hidden_out)\n",
    "        return torch.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a5edecb5-8ad1-4df1-b10b-6fa6e5279e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN classifier\n",
    "class CNN_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, output_dim):\n",
    "        super(CNN_classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        output = self.fc(cat)\n",
    "        return torch.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d64f362e-0a71-4e76-8ae1-eeee2e4db672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training definition for all the models\n",
    "def train_loop(model, train_sentence, train_label, dev_sentence, dev_label, criterion, optimizer, num_epochs, batch_size):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(0, len(train_sentence), batch_size):\n",
    "            #Batching\n",
    "            batch_sentences = train_sentence[i:i + batch_size]\n",
    "            batch_labels = train_label[i:i + batch_size]\n",
    "            batch_sentences = pad_sequence(batch_sentences, batch_first=True, padding_value=0)\n",
    "            batch_labels = torch.tensor(batch_labels, dtype=torch.float32)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_sentences)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            \n",
    "            # Calcul de la perte\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            # Calculer l'accuracy pour le batch\n",
    "            predictions = (outputs > 0.5).long()\n",
    "            correct += (predictions == batch_labels.long()).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "        \n",
    "        # Perte moyenne et précision\n",
    "        train_loss = epoch_loss / len(train_sentence)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        \n",
    "        # Évaluer la précision sur l'ensemble de validation\n",
    "        dev_accuracy = evaluate_model(model, dev_sentence, dev_label, batch_size)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Dev Accuracy: {dev_accuracy:.2f}%\")\n",
    "\n",
    "def evaluate_model(model, dev_sentence, dev_label, batch_size):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dev_sentence), batch_size):\n",
    "            batch_sentences = dev_sentence[i:i + batch_size]\n",
    "            batch_labels = dev_label[i:i + batch_size]\n",
    "            \n",
    "            # Padding\n",
    "            batch_sentences = pad_sequence(batch_sentences, batch_first=True, padding_value=0)\n",
    "            batch_labels = torch.tensor(batch_labels, dtype=torch.float32)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_sentences)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            predictions = (outputs > 0.5).long()\n",
    "            \n",
    "            # Calculer le nombre de prédictions correctes\n",
    "            correct += (predictions == batch_labels.long()).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "    \n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5374a6af-3ffa-4991-addb-281080248851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW\n",
      "Epoch 1/25:\n",
      "Train Loss: 0.0215, Train Accuracy: 54.80%, Dev Accuracy: 61.04%\n",
      "Epoch 2/25:\n",
      "Train Loss: 0.0209, Train Accuracy: 61.69%, Dev Accuracy: 66.80%\n",
      "Epoch 3/25:\n",
      "Train Loss: 0.0202, Train Accuracy: 67.77%, Dev Accuracy: 70.05%\n",
      "Epoch 4/25:\n",
      "Train Loss: 0.0192, Train Accuracy: 72.90%, Dev Accuracy: 72.09%\n",
      "Epoch 5/25:\n",
      "Train Loss: 0.0180, Train Accuracy: 76.36%, Dev Accuracy: 73.85%\n",
      "Epoch 6/25:\n",
      "Train Loss: 0.0167, Train Accuracy: 78.99%, Dev Accuracy: 75.88%\n",
      "Epoch 7/25:\n",
      "Train Loss: 0.0155, Train Accuracy: 81.71%, Dev Accuracy: 76.22%\n",
      "Epoch 8/25:\n",
      "Train Loss: 0.0143, Train Accuracy: 83.64%, Dev Accuracy: 76.83%\n",
      "Epoch 9/25:\n",
      "Train Loss: 0.0133, Train Accuracy: 85.33%, Dev Accuracy: 77.44%\n",
      "Epoch 10/25:\n",
      "Train Loss: 0.0124, Train Accuracy: 86.70%, Dev Accuracy: 78.39%\n",
      "Epoch 11/25:\n",
      "Train Loss: 0.0116, Train Accuracy: 87.80%, Dev Accuracy: 78.46%\n",
      "Epoch 12/25:\n",
      "Train Loss: 0.0109, Train Accuracy: 88.79%, Dev Accuracy: 78.73%\n",
      "Epoch 13/25:\n",
      "Train Loss: 0.0102, Train Accuracy: 89.49%, Dev Accuracy: 79.00%\n",
      "Epoch 14/25:\n",
      "Train Loss: 0.0096, Train Accuracy: 90.06%, Dev Accuracy: 78.79%\n",
      "Epoch 15/25:\n",
      "Train Loss: 0.0091, Train Accuracy: 90.61%, Dev Accuracy: 79.27%\n",
      "Epoch 16/25:\n",
      "Train Loss: 0.0087, Train Accuracy: 91.14%, Dev Accuracy: 79.27%\n",
      "Epoch 17/25:\n",
      "Train Loss: 0.0082, Train Accuracy: 91.67%, Dev Accuracy: 78.93%\n",
      "Epoch 18/25:\n",
      "Train Loss: 0.0078, Train Accuracy: 92.00%, Dev Accuracy: 78.86%\n",
      "Epoch 19/25:\n",
      "Train Loss: 0.0075, Train Accuracy: 92.41%, Dev Accuracy: 79.27%\n",
      "Epoch 20/25:\n",
      "Train Loss: 0.0072, Train Accuracy: 92.71%, Dev Accuracy: 78.66%\n",
      "Epoch 21/25:\n",
      "Train Loss: 0.0069, Train Accuracy: 92.99%, Dev Accuracy: 78.73%\n",
      "Epoch 22/25:\n",
      "Train Loss: 0.0066, Train Accuracy: 93.24%, Dev Accuracy: 78.66%\n",
      "Epoch 23/25:\n",
      "Train Loss: 0.0063, Train Accuracy: 93.50%, Dev Accuracy: 78.52%\n",
      "Epoch 24/25:\n",
      "Train Loss: 0.0061, Train Accuracy: 93.74%, Dev Accuracy: 78.52%\n",
      "Epoch 25/25:\n",
      "Train Loss: 0.0059, Train Accuracy: 94.11%, Dev Accuracy: 78.66%\n"
     ]
    }
   ],
   "source": [
    "#CBOW training\n",
    "print(\"CBOW\")\n",
    "vocab_size = len(my_dico) + 1\n",
    "embedding_dim = 50\n",
    "model = CBOW_classifier(vocab_size, embedding_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 25\n",
    "batch_size = 32\n",
    "train_loop(model, train_sentence, train_label, dev_sentence, dev_label, criterion, optimizer, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "4f543c95-d2ab-40fd-9756-7855aac4c1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW 1 hidden layer\n",
      "Epoch 1/25:\n",
      "Train Loss: 0.0215, Train Accuracy: 53.61%, Dev Accuracy: 58.54%\n",
      "Epoch 2/25:\n",
      "Train Loss: 0.0205, Train Accuracy: 63.70%, Dev Accuracy: 68.16%\n",
      "Epoch 3/25:\n",
      "Train Loss: 0.0180, Train Accuracy: 72.27%, Dev Accuracy: 72.22%\n",
      "Epoch 4/25:\n",
      "Train Loss: 0.0154, Train Accuracy: 77.40%, Dev Accuracy: 76.22%\n",
      "Epoch 5/25:\n",
      "Train Loss: 0.0135, Train Accuracy: 81.16%, Dev Accuracy: 77.37%\n",
      "Epoch 6/25:\n",
      "Train Loss: 0.0119, Train Accuracy: 84.09%, Dev Accuracy: 77.24%\n",
      "Epoch 7/25:\n",
      "Train Loss: 0.0107, Train Accuracy: 86.04%, Dev Accuracy: 77.51%\n",
      "Epoch 8/25:\n",
      "Train Loss: 0.0097, Train Accuracy: 87.61%, Dev Accuracy: 77.91%\n",
      "Epoch 9/25:\n",
      "Train Loss: 0.0088, Train Accuracy: 88.77%, Dev Accuracy: 77.78%\n",
      "Epoch 10/25:\n",
      "Train Loss: 0.0080, Train Accuracy: 90.00%, Dev Accuracy: 78.25%\n",
      "Epoch 11/25:\n",
      "Train Loss: 0.0073, Train Accuracy: 91.20%, Dev Accuracy: 78.05%\n",
      "Epoch 12/25:\n",
      "Train Loss: 0.0067, Train Accuracy: 92.14%, Dev Accuracy: 78.32%\n",
      "Epoch 13/25:\n",
      "Train Loss: 0.0061, Train Accuracy: 93.04%, Dev Accuracy: 78.12%\n",
      "Epoch 14/25:\n",
      "Train Loss: 0.0057, Train Accuracy: 93.73%, Dev Accuracy: 77.71%\n",
      "Epoch 15/25:\n",
      "Train Loss: 0.0052, Train Accuracy: 94.23%, Dev Accuracy: 77.71%\n",
      "Epoch 16/25:\n",
      "Train Loss: 0.0048, Train Accuracy: 94.71%, Dev Accuracy: 77.44%\n",
      "Epoch 17/25:\n",
      "Train Loss: 0.0045, Train Accuracy: 95.11%, Dev Accuracy: 77.37%\n",
      "Epoch 18/25:\n",
      "Train Loss: 0.0042, Train Accuracy: 95.57%, Dev Accuracy: 77.10%\n",
      "Epoch 19/25:\n",
      "Train Loss: 0.0039, Train Accuracy: 95.94%, Dev Accuracy: 76.96%\n",
      "Epoch 20/25:\n",
      "Train Loss: 0.0036, Train Accuracy: 96.01%, Dev Accuracy: 76.83%\n",
      "Epoch 21/25:\n",
      "Train Loss: 0.0034, Train Accuracy: 96.24%, Dev Accuracy: 76.49%\n",
      "Epoch 22/25:\n",
      "Train Loss: 0.0032, Train Accuracy: 96.56%, Dev Accuracy: 76.49%\n",
      "Epoch 23/25:\n",
      "Train Loss: 0.0030, Train Accuracy: 96.80%, Dev Accuracy: 76.36%\n",
      "Epoch 24/25:\n",
      "Train Loss: 0.0029, Train Accuracy: 97.03%, Dev Accuracy: 76.15%\n",
      "Epoch 25/25:\n",
      "Train Loss: 0.0027, Train Accuracy: 97.04%, Dev Accuracy: 76.02%\n"
     ]
    }
   ],
   "source": [
    "#CBOW2 training(one more hidden layer)\n",
    "print(\"CBOW 1 hidden layer\")\n",
    "vocab_size = len(my_dico) + 1\n",
    "embedding_dim = 50\n",
    "model = CBOW_classifier2(vocab_size, embedding_dim, 25)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 25\n",
    "batch_size = 32\n",
    "train_loop(model, train_sentence, train_label, dev_sentence, dev_label, criterion, optimizer, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "237358f8-49c6-46f7-a5a4-5b6da8dee9d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:\n",
      "Train Loss: 0.0204, Train Accuracy: 61.31%, Dev Accuracy: 67.82%\n",
      "Epoch 2/25:\n",
      "Train Loss: 0.0168, Train Accuracy: 74.16%, Dev Accuracy: 72.22%\n",
      "Epoch 3/25:\n",
      "Train Loss: 0.0137, Train Accuracy: 80.84%, Dev Accuracy: 74.05%\n",
      "Epoch 4/25:\n",
      "Train Loss: 0.0110, Train Accuracy: 86.36%, Dev Accuracy: 75.34%\n",
      "Epoch 5/25:\n",
      "Train Loss: 0.0086, Train Accuracy: 90.56%, Dev Accuracy: 75.95%\n",
      "Epoch 6/25:\n",
      "Train Loss: 0.0066, Train Accuracy: 93.53%, Dev Accuracy: 76.22%\n",
      "Epoch 7/25:\n",
      "Train Loss: 0.0050, Train Accuracy: 95.86%, Dev Accuracy: 76.15%\n",
      "Epoch 8/25:\n",
      "Train Loss: 0.0037, Train Accuracy: 97.30%, Dev Accuracy: 76.15%\n",
      "Epoch 9/25:\n",
      "Train Loss: 0.0028, Train Accuracy: 98.19%, Dev Accuracy: 76.42%\n",
      "Epoch 10/25:\n",
      "Train Loss: 0.0021, Train Accuracy: 98.79%, Dev Accuracy: 76.02%\n",
      "Epoch 11/25:\n",
      "Train Loss: 0.0016, Train Accuracy: 99.04%, Dev Accuracy: 75.54%\n",
      "Epoch 12/25:\n",
      "Train Loss: 0.0013, Train Accuracy: 99.30%, Dev Accuracy: 75.27%\n",
      "Epoch 13/25:\n",
      "Train Loss: 0.0010, Train Accuracy: 99.46%, Dev Accuracy: 75.47%\n",
      "Epoch 14/25:\n",
      "Train Loss: 0.0009, Train Accuracy: 99.51%, Dev Accuracy: 75.68%\n",
      "Epoch 15/25:\n",
      "Train Loss: 0.0008, Train Accuracy: 99.53%, Dev Accuracy: 75.95%\n",
      "Epoch 16/25:\n",
      "Train Loss: 0.0006, Train Accuracy: 99.59%, Dev Accuracy: 76.69%\n",
      "Epoch 17/25:\n",
      "Train Loss: 0.0006, Train Accuracy: 99.63%, Dev Accuracy: 76.63%\n",
      "Epoch 18/25:\n",
      "Train Loss: 0.0005, Train Accuracy: 99.69%, Dev Accuracy: 76.69%\n",
      "Epoch 19/25:\n",
      "Train Loss: 0.0005, Train Accuracy: 99.70%, Dev Accuracy: 76.69%\n",
      "Epoch 20/25:\n",
      "Train Loss: 0.0005, Train Accuracy: 99.69%, Dev Accuracy: 76.36%\n",
      "Epoch 21/25:\n",
      "Train Loss: 0.0004, Train Accuracy: 99.70%, Dev Accuracy: 76.42%\n",
      "Epoch 22/25:\n",
      "Train Loss: 0.0004, Train Accuracy: 99.69%, Dev Accuracy: 76.42%\n",
      "Epoch 23/25:\n",
      "Train Loss: 0.0004, Train Accuracy: 99.70%, Dev Accuracy: 76.49%\n",
      "Epoch 24/25:\n",
      "Train Loss: 0.0004, Train Accuracy: 99.73%, Dev Accuracy: 76.83%\n",
      "Epoch 25/25:\n",
      "Train Loss: 0.0004, Train Accuracy: 99.71%, Dev Accuracy: 76.76%\n"
     ]
    }
   ],
   "source": [
    "#CNN training\n",
    "vocab_size = len(my_dico) + 1\n",
    "embedding_dim = 50\n",
    "model = CNN_classifier(vocab_size, embedding_dim, 5, [2, 3, 4], 1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 25\n",
    "batch_size = 32\n",
    "train_loop(model, train_sentence, train_label, dev_sentence, dev_label, criterion, optimizer, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e87607-aa9d-4264-861a-12e2c62a5c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
